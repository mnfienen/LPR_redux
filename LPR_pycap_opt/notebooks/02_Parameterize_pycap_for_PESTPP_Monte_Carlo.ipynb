{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4aaab02-1a82-43b4-aad6-4e642fdfa27f",
   "metadata": {},
   "source": [
    "# Set up Pycap LPR to operate with PEST++ for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52223298-4643-4a71-b29c-14c1830b04b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pyemu\n",
    "import matplotlib.pyplot as plt\n",
    "import os, platform, shutil\n",
    "from pycap.analysis_project import Project\n",
    "from  datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02e584-c9e4-4742-83c2-562c2aa06e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Path to excel input file. Note can use absolute or relative path.\n",
    "pycap_inputs_excel = Path(\"../../PyCap/PyCap_USGS/pycap-dss2/Notebooks/Inputs/LPR_Prepped.xlsx\")\n",
    "\n",
    "#### PyCap Run Name is what all your outputs will have as a name. \n",
    "pycap_run_name = \"LPR_Redux\"\n",
    "\n",
    "#### Base directory for runs\n",
    "parent_run_path = Path(\"../pycap_runs\")\n",
    "\n",
    "#### depletion potential calculations directory\n",
    "base_run_path = parent_run_path / \"pycap_base\"\n",
    "pest_path = parent_run_path / \"pycap_pest\"\n",
    "template_path = pest_path / \"pycap_template\"\n",
    "if pest_path.exists():\n",
    "    shutil.rmtree(pest_path)\n",
    "template_path.mkdir(parents=True)\n",
    "\n",
    "#### finally define the scripts directory\n",
    "script_path = Path(\"../scripts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864fdb7-d721-4740-80cd-f72039e2105d",
   "metadata": {},
   "source": [
    "# Parameterization for PEST++ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b87d00-22d3-450a-af12-df4493dc17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not template_path.exists():\n",
    "    template_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b852186-b3ec-46e0-a295-9ba49c624167",
   "metadata": {},
   "source": [
    "#### let's load up the configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebc87e-131c-4a6c-b89f-74bfa5d39a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_run_path / f\"{pycap_run_name}.yml\", 'r') as ifp:\n",
    "        indat = yaml.safe_load(ifp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e340b4e-03a4-49bb-983e-a4b124f36876",
   "metadata": {},
   "source": [
    "#### and now parameterize inputs to vary in optimization, Monte Carlo, and other analyses\n",
    "\n",
    "First we set up template (`TPL`) files that allow PEST++ to update model input values by name. We do this by reading in the input (`YML`) file and replacing numeric values with updated values being changed by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ffc126-e812-457f-b620-2e92f028344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global T and S\n",
    "T_init = indat['project_properties']['T']\n",
    "S_init = indat['project_properties']['S']\n",
    "\n",
    "indat['project_properties']['T'] = f'~{\"global_T\":^16s}~'\n",
    "indat['project_properties']['S'] = f'~{\"global_S\":^16s}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd61c68-5152-4efd-9159-fe2e80f856ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now well-by-well apportionment and pumping rates\n",
    "well_keys = [i for i in indat.keys() if i.startswith('well_')]\n",
    "app_keys = [[j for j in indat[i].keys() if j.startswith('stream_apportionment')]\n",
    "            for i in well_keys]\n",
    "pending_wells = [i for i in well_keys if 'pending' in indat[i]['status']]\n",
    "allkeys = dict(zip(well_keys, app_keys))\n",
    "\n",
    "pars = list()\n",
    "parvals = list()\n",
    "\n",
    "# for apportionment\n",
    "for k,v in allkeys.items():\n",
    "    for cv in v:\n",
    "        cpar = f'{k}__{cv}'\n",
    "        pars.append(cpar)\n",
    "        parvals.append(indat[k][cv]['apportionment'])\n",
    "        indat[k][cv]['apportionment'] = f'~{cpar:^45}~'\n",
    "# then again for pumping rate Q\n",
    "for k,v in allkeys.items():\n",
    "    cpar = f'{k}__q'\n",
    "    pars.append(cpar)\n",
    "    parvals.append(indat[k]['Q'])\n",
    "    indat[k]['Q'] = f'~{cpar:^45}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7b321-54fb-4978-96d5-87cc1d88139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out tpl file\n",
    "with open(template_path / f\"{pycap_run_name}.yml.tpl\", 'w') as ofp:\n",
    "    ofp.write('ptf ~\\n')\n",
    "    documents = yaml.dump(indat, ofp, default_flow_style = False, sort_keys = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6532c15-ca7a-4241-89be-60d586f6cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame of parameters\n",
    "pars_df = pd.DataFrame(index = pars, data= {'parval1':parvals})\n",
    "\n",
    "# add initial parameters to df\n",
    "pars_df = pd.concat([pars_df, pd.DataFrame(index = ['global_s','global_t'], data = {'parval1':[S_init,T_init]})])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94f0c0-419a-4eee-8442-375b993718e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pars_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316abfa-e814-430c-a0ae-1565eebeb114",
   "metadata": {},
   "source": [
    "### Next we need to be able to read in model ouputs to PEST++\n",
    "Now we write an instruction file (`INS`) that can navigate model output and read it into PEST++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54982082-96d4-4790-9575-49ecb2819777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ins file and external forward run file\n",
    "# set base case depletion observations\n",
    "basedeplobs = [f\"{indat[k]['name']}:bdpl\" for k in indat.keys() if 'stream' in k]\n",
    "\n",
    "# get list of unique stream names used in the run\n",
    "unique_rivers = list(set([i.split(':')[0] for i in basedeplobs]))\n",
    "\n",
    "# add in the totals/sums of proposed/existing/combined depletions for each stream\n",
    "basedeplobs.extend([f'{i}:{j}:bdpl' for i in unique_rivers for j in ['total_proposed','total_existing','total_combined']])\n",
    "\n",
    "with open(template_path / 'basedeplobs.dat','w') as ofp:\n",
    "    [ofp.write(i + '\\n') for i in basedeplobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ab6ea-07b8-4b58-ad68-d85e047bb168",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedeplobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0fa6e-3c0f-4029-b305-4c1d5f5075c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time-series\n",
    "# choose wells with individual time-series outputs.\n",
    "allstrmobs = [f\"{indat[k]['name']}\" for k in indat.keys() if \"stream\" in k]\n",
    "output_ts = allstrmobs \n",
    "\n",
    "# times you want to look at individual outputs, in this case just the depletions in the 5th year\n",
    "times = range(365*4,365*5+1) \n",
    "ts_obs = []\n",
    "for c_ts in output_ts:\n",
    "    ts_obs.extend([f'{c_ts}__{i}' for i in times])\n",
    "allobs = basedeplobs + ts_obs\n",
    "\n",
    "with open(template_path / 'ts_obs.dat' , 'w') as ofp:\n",
    "    [ofp.write(c_ts + '\\n') for c_ts in output_ts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb42fa32-c081-4977-be3b-3ade1d7fc461",
   "metadata": {},
   "source": [
    "### Now read in the base case observation values for depletion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf22cf-20b4-4432-ba0b-76591cc75894",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = pd.read_csv(base_run_path/\"output\" / f'{pycap_run_name}.table_report.base_stream_depletion.csv', index_col=0)\n",
    "# read in the observation names and make a DataFrame to keep the results in\n",
    "bdplobs = pd.read_csv(template_path/'basedeplobs.dat', header=None)\n",
    "bdplobs.columns =['obsname']\n",
    "bdplobs.index = bdplobs.obsname\n",
    "bdplobs['obs_values'] = np.nan\n",
    "\n",
    "# now map the actual output values to the DataFrame\n",
    "for cob in bdplobs.obsname:\n",
    "    riv,wel,_ = cob.split(':')\n",
    "    bdplobs.loc[cob, 'obs_values'] = base_data.loc[wel][riv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d1d9d5-cd7e-4601-835c-0f1755d79a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdplobs.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b74d8-fd32-4147-a739-ee1ea047e929",
   "metadata": {},
   "source": [
    "### Next for time series - read in the results for the 5th year only for each well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22b5a5-0c69-4985-b97f-17c7a047f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data = pd.read_csv(base_run_path / \"output\" / f'{pycap_run_name}.table_report.all_ts.csv',index_col=0)\n",
    "ts_data.columns = ts_data.columns.str.split('__').str[-1]\n",
    "ts_path = template_path / 'ts_obs.dat' \n",
    "output_ts = [i.strip() for i in open(ts_path, 'r').readlines()]\n",
    "ts_df = pd.DataFrame(index= ts_obs,  data = {'obsname':ts_obs, 'obs_values':np.nan})\n",
    "for cob in ts_df.index:\n",
    "    criv,ctime=cob.split('__')\n",
    "    ts_df.loc[cob, 'obs_values'] = ts_data.loc[int(ctime)][criv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36a09f-495f-4f5a-9866-5eb482bbe8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168739d0-b3b8-4c0a-8284-b1f1b8378f06",
   "metadata": {},
   "source": [
    "### We can combine all the outputs into a single dataframe and make the instruction file we'll need to read in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65228136-0e5d-453c-80a6-0a8ec3b813c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "allout = pd.concat([bdplobs,ts_df])\n",
    "allout['obs_values'].to_csv(template_path / 'allobs.out', sep = ' ', header=None)\n",
    "\n",
    "with open(template_path / 'allobs.out.ins', 'w') as ofp:\n",
    "    ofp.write('pif ~\\n')\n",
    "    [ofp.write(f'l1 w !{i}!\\n') for i in allout.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a7d568-b6c2-47de-8a23-f9be0083cb96",
   "metadata": {},
   "source": [
    "### Now we need to make a PEST control file to orchestrate everything. Luckily, `pyemu` makes this straightforward now that we have made the `tpl` and `ins` files --- _Note you may see a warning about parsing metadata. This can be safely ignored._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e854c0-87ba-41ac-a6d6-c065377d43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "os.chdir(template_path)\n",
    "pst = pyemu.Pst.from_io_files(*pyemu.utils.parse_dir_for_io_files('.'))\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78430479-6cee-43f0-9107-b725f72a5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = pst.parameter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f628f1-ce3a-4049-a7d9-f8f391c82d82",
   "metadata": {},
   "source": [
    "# let's clean up some of the data and add important values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b85c1-d77d-49b9-9cda-4487a6ed2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name paramter groups according to the type of parameter\n",
    "pars.loc[pars.parnme.str.contains(\"global\"), \"pargp\"] = \"global\"\n",
    "pars.loc[pars.parnme.str.endswith(\"q\"), \"pargp\"] = \"pumping\"\n",
    "pars.loc[pars.parnme.str.contains(\"stream\"), \"pargp\"] = \"apportionment\"\n",
    "# set initial values\n",
    "pars.loc[pars_df.index,'parval1'] = pars_df.parval1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c258c0-ae92-4852-b625-77623c77a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_pump = .2 #(as a fraction)\n",
    "# set upper and lower bounds on pumping rates as well\n",
    "pars.loc[pars.pargp==\"pumping\",\"parlbnd\"] = pars.loc[pars.pargp==\"pumping\", \"parval1\"]*(1-del_pump)\n",
    "pars.loc[pars.pargp==\"pumping\", \"parubnd\"] = pars.loc[pars.pargp==\"pumping\", \"parval1\"]*(1+del_pump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30868514-a1dd-411e-a942-9ba90f6bc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set apportionment changes\n",
    "# how much to change apportionment\n",
    "del_apport = 0.1\n",
    "\n",
    "# apport lower and upper bound\n",
    "#pars.loc[pars.index.str.startswith('well_'),'parlbnd'] = pars.loc[pars.index.str.startswith('well_'),'parval1']-.1\n",
    "pars.loc[pars.pargp==\"apportionment\", \"parlbnd\"] = pars.loc[pars.pargp==\"apportionment\", \"parval1\"] - del_apport\n",
    "pars.loc[pars.pargp==\"apportionment\", \"parubnd\"] = pars.loc[pars.pargp==\"apportionment\", \"parval1\"] + del_apport\n",
    "\n",
    "# force overall lowerbound and upperbound as defined in .xlsx inputs\n",
    "pars.loc[(pars.pargp=='apportionment')&\n",
    "    (pars.parlbnd < indat['project_properties']['Min_FracInt']),\n",
    "    'parlbnd'] = indat['project_properties']['Min_FracInt']\n",
    "pars.loc[(pars.pargp=='apportionment')&\n",
    "    (pars.parubnd > indat['project_properties']['Max_FracInt']),\n",
    "    'parubnd'] = indat['project_properties']['Max_FracInt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d4189-365a-4618-8f4a-e4b7c2911083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set lower and upper bounds for T and S as defined in .xlsx inputs\n",
    "pars.loc[(pars.parnme=='global_t'),'parlbnd'] = indat['project_properties']['Min_T']\n",
    "pars.loc[(pars.parnme=='global_t'),'parubnd'] = indat['project_properties']['Max_T']\n",
    "\n",
    "pars.loc[(pars.parnme=='global_s'),'parlbnd'] = indat['project_properties']['Min_S']\n",
    "pars.loc[(pars.parnme=='global_s'),'parubnd'] = indat['project_properties']['Max_S']\n",
    "\n",
    "# set transforms for all parameters to 'none', except for T which can be a log-transform\n",
    "pars.partrans = 'none'\n",
    "pars.loc['global_t','partrans']='log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb74b81-bb3d-499d-9c40-664f4448ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and save pst file\n",
    "pst.control_data.noptmax = -1\n",
    "pst.model_command = [f'python run_pycap_standalone.py']\n",
    "pst.pestpp_options['par_sigma_range']=6\n",
    "pst.pestpp_options['ies_num_reals'] = 50\n",
    "\n",
    "pst.write(str(template_path / 'prior_mc.pst'), version=2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
